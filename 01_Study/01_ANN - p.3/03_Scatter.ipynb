{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인공신경 간단하게 실습 100 epoch 마다 산점도 시각화 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인공신경망 모델 클래스 정의\n",
    "class SimpleNet(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    입력층 2개, 은닉층 1개(노드 5), 출력층 1개 (sigmoid 활성화 함수 사용)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SimpleNet,self).__init__()\n",
    "        self.fc1 = nn.Linear(2,5)   # 입력 2 -> 노드 5\n",
    "        self.fc1 = nn.Linear(5,1)   # 노드 5 -> 출력 1\n",
    "        \n",
    "        pass\n",
    "    def __forward__(self):\n",
    "        x = torch.relu(self.fc1(x)) # relu 입력\n",
    "        x = torch.sigmoid(self.fc2(x))  # sigmoid 출력\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2118,  1.0080],\n",
      "        [ 1.6448,  0.3886],\n",
      "        [-0.0740, -0.9735],\n",
      "        [ 0.7476,  1.2778],\n",
      "        [ 2.0646,  1.1799],\n",
      "        [ 1.2352,  1.3772],\n",
      "        [-0.8043,  0.8470],\n",
      "        [ 1.1131,  0.7183],\n",
      "        [-0.4600, -0.6620],\n",
      "        [ 0.6589,  0.7450],\n",
      "        [-0.1778, -0.5370],\n",
      "        [-1.6556, -1.9073],\n",
      "        [-0.4506,  0.1555],\n",
      "        [ 1.2180, -0.4233],\n",
      "        [ 0.6898,  0.2328],\n",
      "        [ 0.2706,  1.0099],\n",
      "        [ 0.0250, -0.8766],\n",
      "        [ 0.0544, -0.8479],\n",
      "        [ 0.1479, -1.3520],\n",
      "        [-0.2327, -0.8913],\n",
      "        [-1.2788,  0.3870],\n",
      "        [-1.2082, -0.1724],\n",
      "        [-1.6433, -2.4174],\n",
      "        [ 1.7456,  0.2024],\n",
      "        [ 0.0703, -0.0040],\n",
      "        [ 0.0638,  0.7603],\n",
      "        [-0.1622,  0.8638],\n",
      "        [-1.6427,  0.2580],\n",
      "        [-0.0051, -0.2314],\n",
      "        [ 0.7359, -0.1580],\n",
      "        [ 0.0869, -1.5483],\n",
      "        [ 1.0251, -0.0624],\n",
      "        [-0.8493, -0.2610],\n",
      "        [-0.2835,  0.4031],\n",
      "        [ 0.3120,  0.2270],\n",
      "        [-0.5757, -2.2901],\n",
      "        [-0.1315, -1.6455],\n",
      "        [-1.1169, -1.5585],\n",
      "        [-0.2190,  0.3085],\n",
      "        [ 0.6030,  1.0850],\n",
      "        [-1.2680, -1.3562],\n",
      "        [ 1.0234, -0.8944],\n",
      "        [ 0.4507,  0.2894],\n",
      "        [-1.1486,  0.0721],\n",
      "        [ 0.3725,  1.3783],\n",
      "        [-0.4916,  0.5139],\n",
      "        [ 1.5809, -0.1781],\n",
      "        [-0.1707, -0.9549],\n",
      "        [-1.1743,  1.0828],\n",
      "        [-0.7178,  0.6850],\n",
      "        [-1.8543, -1.2748],\n",
      "        [-1.9151,  0.7330],\n",
      "        [-0.3387,  0.2284],\n",
      "        [-1.0808,  2.6709],\n",
      "        [-0.4678,  0.3992],\n",
      "        [ 0.2330, -0.3960],\n",
      "        [ 0.2384,  1.1120],\n",
      "        [ 0.3821,  1.8516],\n",
      "        [-0.2310, -0.9163],\n",
      "        [ 0.5289,  0.6926],\n",
      "        [ 2.0247,  0.2156],\n",
      "        [ 0.0674,  2.8056],\n",
      "        [-1.2772,  0.1322],\n",
      "        [ 0.3563,  0.6517],\n",
      "        [ 0.3559,  0.1762],\n",
      "        [-1.8931, -0.8375],\n",
      "        [-1.1314, -0.0064],\n",
      "        [-1.1370,  1.0423],\n",
      "        [ 0.9068,  1.1495],\n",
      "        [ 1.2424,  0.0054],\n",
      "        [ 1.0092,  0.0577],\n",
      "        [ 0.2359, -0.6352],\n",
      "        [-0.9487,  0.8835],\n",
      "        [ 0.7056,  0.2297],\n",
      "        [ 0.2054,  1.4854],\n",
      "        [ 0.2492,  1.1089],\n",
      "        [ 0.0592, -0.0045],\n",
      "        [-0.4002, -0.7206],\n",
      "        [-1.6395, -0.1955],\n",
      "        [ 1.5219,  0.2235],\n",
      "        [-1.2707,  0.1847],\n",
      "        [-0.2647,  1.2321],\n",
      "        [ 0.0074, -1.0736],\n",
      "        [ 0.9807,  0.2323],\n",
      "        [-0.0579, -0.7658],\n",
      "        [ 0.6402,  0.5095],\n",
      "        [ 0.7010,  0.7421],\n",
      "        [-0.2860, -0.2686],\n",
      "        [-1.1785, -0.9976],\n",
      "        [-0.5187, -0.8068],\n",
      "        [ 0.7316, -0.6215],\n",
      "        [-0.4786,  0.7645],\n",
      "        [-0.8141,  0.2086],\n",
      "        [ 0.2208,  1.5327],\n",
      "        [ 1.3692, -0.5120],\n",
      "        [ 1.0489, -0.5206],\n",
      "        [-1.0706, -0.9577],\n",
      "        [-0.4475, -0.9456],\n",
      "        [ 0.0843,  0.1910],\n",
      "        [ 0.3977,  0.8740]])\n",
      "tensor([1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
      "        1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0.,\n",
      "        1., 0., 0., 1., 1., 1., 0., 0., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(100,2)\n",
    "y = ((x[:,0])>0 & (x[:,1]>0)).float()   # true,false -> 정답지\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNet(\n",
      "  (fc1): Linear(in_features=5, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = SimpleNet()\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss 함수와 optimizer 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Module [SimpleNet] is missing the required \"forward\" function",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1000\u001b[39m) :\n\u001b[1;32m      2\u001b[0m     \n\u001b[1;32m      3\u001b[0m     \u001b[39m# 순전파 단계\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     y_pred \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m      5\u001b[0m     \u001b[39mprint\u001b[39m(y_pred)\n\u001b[1;32m      7\u001b[0m     loss \u001b[39m=\u001b[39m criterion(y_pred\u001b[39m.\u001b[39msqueeze(),y)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:201\u001b[0m, in \u001b[0;36m_forward_unimplemented\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward_unimplemented\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39minput\u001b[39m: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Defines the computation performed at every call.\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \n\u001b[1;32m    193\u001b[0m \u001b[39m    Should be overridden by all subclasses.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[39m        registered hooks while the latter silently ignores them.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 201\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModule [\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m] is missing the required \u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39mforward\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m function\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Module [SimpleNet] is missing the required \"forward\" function"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000) :\n",
    "    \n",
    "    # 순전파 단계\n",
    "    y_pred = model(x)\n",
    "    print(y_pred)\n",
    "    \n",
    "    loss = criterion(y_pred.squeeze(),y)\n",
    "    \n",
    "    # 역전파 단계\n",
    "    optimizer.zero_grad()\n",
    "    # optimizer의 모든 가중치에 대한 변화(gradient) 0으로 초기화\n",
    "    # -> 이전 기울기 값이 남아 있을 수 있음\n",
    "    \n",
    "    loss.backward()     # loss 함수에 대해 역전파 수행\n",
    "    \n",
    "    optimizer.step()    # optimizer 가중치 업데이트 -> 다음 스텝으로\n",
    "    \n",
    "    if epoch % 10 == 0 :\n",
    "        plt.clf()\n",
    "        plt.scatter(x[:,0],x[:,1], c=y_pred.detach().numpy().sqeeze())\n",
    "        plt.show()\n",
    "    print('학습완료')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
